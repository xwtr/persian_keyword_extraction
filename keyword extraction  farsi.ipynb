{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import nltk\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,TFBertModel,AutoTokenizer,AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=['.', 'از', 'برای', 'در','با','ما','همه','[SEP]','[CLS]','است', 'کنند']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=\"\"\"تهران (دربارهٔ این پرونده آوا راهنما·اطلاعات) پرجمعیت‌ترین شهر و پایتخت ایران، مرکز استان تهران و شهرستان تهران است. این شهر با ۸٬۶۹۳٬۷۰۶ تن جمعیت، بیست و چهارمین شهر پرجمعیت جهان و پرجمعیت‌ترین شهر باختر آسیا به‌شمار می‌رود. کلان‌شهر تهران نیز دومین کلان‌شهر پرجمعیت خاورمیانه است.. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n=10\n",
    "nr_candidate=30\n",
    "model_name_or_path = \"HooshvareLab/bert-fa-zwnj-base\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get word and text embedding using <a hert=\"https://github.com/hooshvare/parsbert\">parsbert</a>\n",
    "\n",
    "text embedding = mean(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def  bert_embeder(model_name_or_path,doc,stop_words):\n",
    "    config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name_or_path)\n",
    "    \n",
    "    model = TFBertModel.from_pretrained(model_name_or_path)\n",
    "    count=CountVectorizer(ngram_range=(1,1),stop_words=stop_words).fit([doc])\n",
    "    encoded_input=tokenizer.encode(count.get_feature_names())\n",
    "    input_ids = tf.constant(encoded_input)[None, :]  # Batch size 1\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0]\n",
    "    candidate_embedding=last_hidden_states[0].numpy()\n",
    "    doc_embedding=np.mean(last_hidden_states.numpy(),axis=1)\n",
    "    \n",
    "    candidate=tokenizer.tokenize(tokenizer.decode(encoded_input))\n",
    "\n",
    "    return doc_embedding,candidate_embedding,candidate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def max_sum_sim(doc_embedding, candidate_embeddings, candidate, top_n, nr_candidate):\n",
    "    # Calculate distances and extract keywords\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    distances_candidate = cosine_similarity(candidate_embeddings, \n",
    "                                            candidate_embeddings)\n",
    "\n",
    "    # Get top_n words as candidate based on cosine similarity\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidate:])\n",
    "    words_vals = [candidate[index] for index in words_idx]\n",
    "    distances_candidate = distances_candidate[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # Calculate the combination of words that are the least similar to each other\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidate[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmr(doc_embedding, word_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # Extract similarity within words, and between words and the document\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphras\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at HooshvareLab/bert-fa-zwnj-base were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertModel were not initialized from the model checkpoint at HooshvareLab/bert-fa-zwnj-base and are newly initialized: ['bert/pooler/dense/kernel:0', 'bert/pooler/dense/bias:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cls', 'sep'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "doc_embedding,candidate_embedding,candidate=bert_embeder(model_name_or_path,doc,stop_words)\n",
    "\n",
    "\n",
    "distances=cosine_similarity(doc_embedding,candidate_embedding)\n",
    "\n",
    "keywords=[candidate[index] for index in distances.argsort()[0][-top_n:]]\n",
    "high_mmr=mmr(doc_embedding,candidate_embedding,candidate,top_n,diversity=0.7)\n",
    "low_mmr=mmr(doc_embedding,candidate_embedding,candidate,top_n,diversity=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['پرجمعیت', '[UNK]', 'تهران', 'خاورمیانه', 'شهر', '[UNK]', 'پرونده', 'راهنما', 'پایتخت', 'دومین']\n"
     ]
    }
   ],
   "source": [
    "print(low_mmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['پرجمعیت', '[UNK]', 'نیز', 'اطلاعات', 'تن', 'این', 'مرکز', 'به', 'بیست', 'رود']\n"
     ]
    }
   ],
   "source": [
    "print(high_mmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
